{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\21265\\anaconda3\\envs\\tfpy39\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "import tensorflow_addons as tfa \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text data <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None, 50), (None, 5)), types: (tf.int32, tf.float32)>,\n",
       " <PrefetchDataset shapes: ((None, 50), (None, 5)), types: (tf.int32, tf.float32)>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_ds = tf.data.experimental.load('./data/text/train_ds/').shuffle(buffer_size=1000, seed=42).batch(batch_size=32).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "valid_ds = tf.data.experimental.load('./data/text/valid_ds/').shuffle(buffer_size=1000, seed=42).batch(batch_size=32).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds  = tf.data.experimental.load('./data/text/test_ds/').shuffle(buffer_size=1000, seed=42).batch(batch_size=32).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = np.load('./data/text/embed_matrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 11052\n",
    "sentlen    = 50\n",
    " \n",
    "inputs = keras.layers.Input(shape=(sentlen))\n",
    "embed  = keras.layers.Embedding(input_dim=vocab_size, output_dim=100, embeddings_initializer=keras.initializers.Constant(embed_matrix),input_length=sentlen, trainable=False)(inputs)\n",
    "\n",
    "x = keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu')(embed)\n",
    "x = keras.layers.Conv1D(filters=8, kernel_size=3, activation='relu')(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(50, activation='relu')(x)\n",
    "\n",
    "y = keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu')(embed)\n",
    "y = keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu')(y)\n",
    "y = keras.layers.Flatten()(y)\n",
    "y = keras.layers.Dense(50, activation='relu')(y)\n",
    "\n",
    "z = keras.layers.Concatenate()([x,y])\n",
    "\n",
    "z = keras.layers.Dense(256, activation='relu')(z)\n",
    "z = keras.layers.Dense(5, activation='sigmoid')(z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text_model = keras.models.Model(inputs=inputs, outputs=z, name='text_model')\n",
    "text_model.compile(loss='mse', optimizer=tfa.optimizers.RectifiedAdam(), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile & Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "188/188 [==============================] - 11s 34ms/step - loss: 0.0220 - mae: 0.1194 - val_loss: 0.0208 - val_mae: 0.1156\n",
      "Epoch 2/100\n",
      "188/188 [==============================] - 6s 32ms/step - loss: 0.0208 - mae: 0.1158 - val_loss: 0.0206 - val_mae: 0.1149\n",
      "Epoch 3/100\n",
      "188/188 [==============================] - 7s 35ms/step - loss: 0.0198 - mae: 0.1128 - val_loss: 0.0209 - val_mae: 0.1152\n",
      "Epoch 4/100\n",
      "188/188 [==============================] - 7s 35ms/step - loss: 0.0182 - mae: 0.1079 - val_loss: 0.0218 - val_mae: 0.1178\n",
      "Epoch 5/100\n",
      "188/188 [==============================] - 7s 35ms/step - loss: 0.0156 - mae: 0.0997 - val_loss: 0.0236 - val_mae: 0.1227\n",
      "Epoch 6/100\n",
      "188/188 [==============================] - 7s 35ms/step - loss: 0.0126 - mae: 0.0890 - val_loss: 0.0256 - val_mae: 0.1278\n",
      "Epoch 7/100\n",
      "188/188 [==============================] - 7s 35ms/step - loss: 0.0104 - mae: 0.0809 - val_loss: 0.0269 - val_mae: 0.1306\n",
      "Epoch 8/100\n",
      "188/188 [==============================] - 6s 34ms/step - loss: 0.0110 - mae: 0.0832 - val_loss: 0.0289 - val_mae: 0.1359\n",
      "Epoch 9/100\n",
      "188/188 [==============================] - 4s 22ms/step - loss: 0.0103 - mae: 0.0808 - val_loss: 0.0281 - val_mae: 0.1336\n",
      "Epoch 10/100\n",
      "188/188 [==============================] - 6s 30ms/step - loss: 0.0088 - mae: 0.0744 - val_loss: 0.0284 - val_mae: 0.1347\n",
      "Epoch 11/100\n",
      "188/188 [==============================] - 6s 32ms/step - loss: 0.0079 - mae: 0.0704 - val_loss: 0.0301 - val_mae: 0.1383\n",
      "Epoch 12/100\n",
      "188/188 [==============================] - 6s 32ms/step - loss: 0.0077 - mae: 0.0695 - val_loss: 0.0260 - val_mae: 0.1285\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "t = datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=10, verbose=0)\n",
    "check_point    = keras.callbacks.ModelCheckpoint(filepath='./weights/text/'+str(t)+'/text.t5',\n",
    "                             monitor='val_mae',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             verbose=0)\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam()\n",
    "\n",
    "history = text_model.fit(train_ds, validation_data=valid_ds, batch_size=32, epochs=100, callbacks=[early_stopping, check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1da04ec27c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.load_weights('./weights/text/0226_072643/text.t5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None, 50), (None, 5)), types: (tf.int32, tf.float32)>,\n",
       " <PrefetchDataset shapes: ((None, 50), (None, 5)), types: (tf.int32, tf.float32)>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "valid_ds = tf.data.experimental.load('./data/text/valid_ds/').batch(batch_size=32).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds  = tf.data.experimental.load('./data/text/test_ds/').batch(batch_size=32).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "valid_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([88.668   , 87.64489 , 88.154854, 89.908455, 88.18838 ],\n",
       "       dtype=float32),\n",
       " 88.51291686296463)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.concatenate([y for x,y in valid_ds], axis=0)\n",
    "y_pred = text_model.predict(valid_ds)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "(1-mae)*100, (1-np.mean(mae))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 3s 3ms/step - loss: 0.0206 - mae: 0.1149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88.51291239261627"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, mae = text_model.evaluate(valid_ds)\n",
    "(1-mae)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([88.529396, 87.800285, 87.93243 , 89.40246 , 88.01859 ],\n",
       "       dtype=float32),\n",
       " 88.33663240075111)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.concatenate([y for x,y in test_ds], axis=0)\n",
    "y_pred = text_model.predict(test_ds)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "(1-mae)*100, (1-np.mean(mae))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0208 - mae: 0.1166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88.33663538098335"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, mae = text_model.evaluate(test_ds)\n",
    "(1-mae)*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfpy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
