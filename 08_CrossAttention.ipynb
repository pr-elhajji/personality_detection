{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "import tensorflow_addons as tfa \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from models import load_scene_model, load_face_model, load_audio_model, load_text_glove_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_model = load_scene_model()\n",
    "face_model  = load_face_model()\n",
    "audio_model = load_audio_model()\n",
    "text_model  = load_text_glove_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"scene_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Scene_Input (InputLayer)        [(None, 10, 224, 224 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Scene_Rescaling (TimeDistribute (None, 10, 224, 224, 0           Scene_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Scene_time_distributed (TimeDis (None, 10, 224, 224, 0           Scene_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Scene_vgg16 (TimeDistributed)   (None, 10, 512)      14714688    Scene_Rescaling[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Scene_time_distributed_1 (TimeD (None, 10, 768)      85798656    Scene_time_distributed[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Scene_lstm (LSTM)               (None, 10, 128)      328192      Scene_vgg16[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Scene_lstm_2 (LSTM)             (None, 10, 128)      459264      Scene_time_distributed_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Scene_lstm_1 (LSTM)             (None, 64)           49408       Scene_lstm[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Scene_lstm_3 (LSTM)             (None, 64)           49408       Scene_lstm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dropout (Dropout)         (None, 64)           0           Scene_lstm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dropout_1 (Dropout)       (None, 64)           0           Scene_lstm_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dense (Dense)             (None, 1024)         66560       Scene_dropout[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dense_2 (Dense)           (None, 1024)         66560       Scene_dropout_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dense_1 (Dense)           (None, 512)          524800      Scene_dense[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dense_3 (Dense)           (None, 512)          524800      Scene_dense_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Scene_average (Average)         (None, 512)          0           Scene_dense_1[0][0]              \n",
      "                                                                 Scene_dense_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dense_4 (Dense)           (None, 256)          131328      Scene_average[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dropout_2 (Dropout)       (None, 256)          0           Scene_dense_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dense_5 (Dense)           (None, 5)            1285        Scene_dropout_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 102,714,949\n",
      "Trainable params: 0\n",
      "Non-trainable params: 102,714,949\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in scene_model.layers:\n",
    "    layer.trainable = False \n",
    "    layer._name = 'Scene_' + layer._name\n",
    "scene_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"face_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Face_Input (InputLayer)         [(None, 10, 224, 224 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Face_Rescaling (TimeDistributed (None, 10, 224, 224, 0           Face_Input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Face_time_distributed_2 (TimeDi (None, 10, 224, 224, 0           Face_Input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Face_vgg16 (TimeDistributed)    (None, 10, 512)      14714688    Face_Rescaling[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Face_time_distributed_3 (TimeDi (None, 10, 768)      85798656    Face_time_distributed_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Face_lstm_4 (LSTM)              (None, 10, 128)      328192      Face_vgg16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Face_lstm_6 (LSTM)              (None, 10, 128)      459264      Face_time_distributed_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Face_lstm_5 (LSTM)              (None, 64)           49408       Face_lstm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Face_lstm_7 (LSTM)              (None, 64)           49408       Face_lstm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Face_dropout_3 (Dropout)        (None, 64)           0           Face_lstm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Face_dropout_4 (Dropout)        (None, 64)           0           Face_lstm_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Face_dense_6 (Dense)            (None, 1024)         66560       Face_dropout_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Face_dense_8 (Dense)            (None, 1024)         66560       Face_dropout_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Face_dense_7 (Dense)            (None, 512)          524800      Face_dense_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Face_dense_9 (Dense)            (None, 512)          524800      Face_dense_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Face_average_1 (Average)        (None, 512)          0           Face_dense_7[0][0]               \n",
      "                                                                 Face_dense_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Face_dense_10 (Dense)           (None, 256)          131328      Face_average_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Face_dropout_5 (Dropout)        (None, 256)          0           Face_dense_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Face_dense_11 (Dense)           (None, 5)            1285        Face_dropout_5[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 102,714,949\n",
      "Trainable params: 0\n",
      "Non-trainable params: 102,714,949\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in face_model.layers:\n",
    "    layer.trainable = False \n",
    "    layer._name = 'Face_' + layer._name\n",
    "face_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"audio_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Audio_input_2 (InputLayer)   [(None, 15, 128)]         0         \n",
      "_________________________________________________________________\n",
      "Audio_conv1d (Conv1D)        (None, 14, 32)            8224      \n",
      "_________________________________________________________________\n",
      "Audio_dropout_6 (Dropout)    (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "Audio_conv1d_1 (Conv1D)      (None, 13, 64)            4160      \n",
      "_________________________________________________________________\n",
      "Audio_dropout_7 (Dropout)    (None, 13, 64)            0         \n",
      "_________________________________________________________________\n",
      "Audio_lstm_8 (LSTM)          (None, 13, 512)           1181696   \n",
      "_________________________________________________________________\n",
      "Audio_lstm_9 (LSTM)          (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "Audio_dense_12 (Dense)       (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "Audio_dropout_8 (Dropout)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Audio_dense_13 (Dense)       (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 2,048,613\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,048,613\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in audio_model.layers:\n",
    "    layer.trainable = False\n",
    "    layer._name = 'Audio_' + layer._name\n",
    "audio_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Text_input_3 (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Text_embedding (Embedding)      (None, 50, 100)      1105200     Text_input_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Text_conv1d_2 (Conv1D)          (None, 48, 16)       4816        Text_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Text_conv1d_4 (Conv1D)          (None, 48, 32)       9632        Text_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Text_conv1d_3 (Conv1D)          (None, 46, 8)        392         Text_conv1d_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Text_conv1d_5 (Conv1D)          (None, 46, 16)       1552        Text_conv1d_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Text_flatten (Flatten)          (None, 368)          0           Text_conv1d_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Text_flatten_1 (Flatten)        (None, 736)          0           Text_conv1d_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Text_dense_14 (Dense)           (None, 50)           18450       Text_flatten[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Text_dense_15 (Dense)           (None, 50)           36850       Text_flatten_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Text_concatenate (Concatenate)  (None, 100)          0           Text_dense_14[0][0]              \n",
      "                                                                 Text_dense_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Text_dense_16 (Dense)           (None, 256)          25856       Text_concatenate[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Text_dense_17 (Dense)           (None, 5)            1285        Text_dense_16[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,204,033\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,204,033\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in text_model.layers:\n",
    "    layer.trainable = False \n",
    "    layer._name = 'Text_' + layer._name\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_inputs = keras.layers.Input(shape=(10,224,224,3), name='Scene_input')\n",
    "face_inputs  = keras.layers.Input(shape=(10,224,224,3), name='Face_input')\n",
    "audio_inputs = keras.layers.Input(shape=(15,128), name='Audio_input')\n",
    "text_inputs  = keras.layers.Input(shape=(50), name='Text_input')\n",
    "\n",
    "x = scene_model.layers[-13].output\n",
    "y = face_model.layers[-13].output\n",
    "\n",
    "v = keras.layers.Average()([x,y])\n",
    "v = keras.layers.Dense(64, activation='relu')(v)\n",
    "\n",
    "a = audio_model.layers[-6].output\n",
    "t = text_model.layers[-8].output\n",
    "t = keras.layers.Dense(64, activation='relu')(t)\n",
    "\n",
    "a1 = keras.layers.MultiHeadAttention(num_heads=2, key_dim=64)(v,a) # video->audio\n",
    "a2 = keras.layers.MultiHeadAttention(num_heads=2, key_dim=64)(a,v) # audio->video\n",
    "\n",
    "a3 = keras.layers.MultiHeadAttention(num_heads=2, key_dim=64)(v,t) # video->text\n",
    "a4 = keras.layers.MultiHeadAttention(num_heads=2, key_dim=64)(t,v) # text->video\n",
    "\n",
    "a5 = keras.layers.MultiHeadAttention(num_heads=2, key_dim=64)(a,t) # audio->text\n",
    "a6 = keras.layers.MultiHeadAttention(num_heads=2, key_dim=64)(t, a) # text-audio\n",
    "\n",
    "\n",
    "\n",
    "o = keras.layers.Concatenate(axis=1)([a1, a2, a3, a4, a5, a6])\n",
    "o = keras.layers.GlobalAveragePooling1D()(o)\n",
    "\n",
    "\n",
    "o = keras.layers.Dense(64, activation='relu')(o)\n",
    "o = keras.layers.Dense(5, activation='sigmoid')(o)\n",
    "\n",
    "atten_model = keras.models.Model(inputs=[scene_model.input, face_model.input, audio_model.input, text_model.input], outputs=o)\n",
    "\n",
    "atten_model.compile(loss='mse', optimizer=tfa.optimizers.RectifiedAdam(), metrics=['mae'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "t = datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=10, verbose=0)\n",
    "check_point    = keras.callbacks.ModelCheckpoint(filepath='./weights/cross_atten/'+str(t)+'/attention.t5',\n",
    "                             monitor='val_mae',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>,\n",
       " <PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Train\n",
    "scene_train_ds = tf.data.experimental.load('./data/fullscene/train_ds/')\n",
    "face_train_ds  = tf.data.experimental.load('./data/faces/train_ds/')\n",
    "audio_train_ds = tf.data.experimental.load('./data/audio/train_ds/')\n",
    "text_train_ds  = tf.data.experimental.load('./data/text/train_ds/').batch(batch_size=32)\n",
    "\n",
    "scene_xtrain = scene_train_ds.map(lambda x,y: x)\n",
    "face_xtrain  = face_train_ds.map(lambda x,y: x)\n",
    "audio_xtrain = audio_train_ds.map(lambda x,y: x)\n",
    "text__xtrain = text_train_ds.map(lambda x,y: x)\n",
    "y_train      = scene_train_ds.map(lambda x,y: y)\n",
    "\n",
    "train_ds = tf.data.Dataset.zip(((scene_xtrain, face_xtrain, audio_xtrain, text__xtrain), y_train)).shuffle(buffer_size=1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "# Valid\n",
    "scene_valid_ds = tf.data.experimental.load('./data/fullscene/valid_ds/')\n",
    "face_valid_ds  = tf.data.experimental.load('./data/faces/valid_ds/')\n",
    "audio_valid_ds = tf.data.experimental.load('./data/audio/valid_ds') \n",
    "text_valid_ds  = tf.data.experimental.load('./data/text/valid_ds/').batch(batch_size=32)\n",
    "\n",
    "scene_xvalid = scene_valid_ds.map(lambda x,y: x)\n",
    "face_xvalid  = face_valid_ds.map(lambda x,y: x)\n",
    "audio_xvalid = audio_valid_ds.map(lambda x,y: x)\n",
    "text_xvalid  = text_valid_ds.map(lambda x,y: x)\n",
    "y_valid      = scene_valid_ds.map(lambda x,y: y)\n",
    "\n",
    "valid_ds = tf.data.Dataset.zip(((scene_xvalid, face_xvalid, audio_xvalid, text_xvalid), y_valid)).shuffle(buffer_size=1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "188/188 [==============================] - 560s 2s/step - loss: 0.0155 - mae: 0.0986 - val_loss: 0.0123 - val_mae: 0.0877\n",
      "Epoch 2/100\n",
      "188/188 [==============================] - 473s 2s/step - loss: 0.0076 - mae: 0.0694 - val_loss: 0.0117 - val_mae: 0.0855\n",
      "Epoch 3/100\n",
      "188/188 [==============================] - 459s 2s/step - loss: 0.0068 - mae: 0.0656 - val_loss: 0.0118 - val_mae: 0.0857\n",
      "Epoch 4/100\n",
      "188/188 [==============================] - 461s 2s/step - loss: 0.0065 - mae: 0.0642 - val_loss: 0.0116 - val_mae: 0.0850\n",
      "Epoch 5/100\n",
      "188/188 [==============================] - 455s 2s/step - loss: 0.0063 - mae: 0.0631 - val_loss: 0.0114 - val_mae: 0.0845\n",
      "Epoch 6/100\n",
      "188/188 [==============================] - 452s 2s/step - loss: 0.0061 - mae: 0.0621 - val_loss: 0.0112 - val_mae: 0.0839\n",
      "Epoch 7/100\n",
      "188/188 [==============================] - 460s 2s/step - loss: 0.0059 - mae: 0.0609 - val_loss: 0.0110 - val_mae: 0.0830\n",
      "Epoch 8/100\n",
      "188/188 [==============================] - 442s 2s/step - loss: 0.0058 - mae: 0.0607 - val_loss: 0.0113 - val_mae: 0.0840\n",
      "Epoch 9/100\n",
      "188/188 [==============================] - 440s 2s/step - loss: 0.0057 - mae: 0.0599 - val_loss: 0.0112 - val_mae: 0.0837\n",
      "Epoch 10/100\n",
      "188/188 [==============================] - 442s 2s/step - loss: 0.0056 - mae: 0.0593 - val_loss: 0.0113 - val_mae: 0.0842\n",
      "Epoch 11/100\n",
      "188/188 [==============================] - 435s 2s/step - loss: 0.0055 - mae: 0.0587 - val_loss: 0.0114 - val_mae: 0.0846\n",
      "Epoch 12/100\n",
      "188/188 [==============================] - 439s 2s/step - loss: 0.0054 - mae: 0.0585 - val_loss: 0.0112 - val_mae: 0.0838\n",
      "Epoch 13/100\n",
      "188/188 [==============================] - 443s 2s/step - loss: 0.0053 - mae: 0.0580 - val_loss: 0.0113 - val_mae: 0.0841\n",
      "Epoch 14/100\n",
      "188/188 [==============================] - 439s 2s/step - loss: 0.0053 - mae: 0.0576 - val_loss: 0.0115 - val_mae: 0.0848\n",
      "Epoch 15/100\n",
      "188/188 [==============================] - 445s 2s/step - loss: 0.0052 - mae: 0.0570 - val_loss: 0.0119 - val_mae: 0.0860\n",
      "Epoch 16/100\n",
      "188/188 [==============================] - 449s 2s/step - loss: 0.0051 - mae: 0.0568 - val_loss: 0.0112 - val_mae: 0.0836\n",
      "Epoch 17/100\n",
      "188/188 [==============================] - 463s 2s/step - loss: 0.0050 - mae: 0.0561 - val_loss: 0.0111 - val_mae: 0.0834\n"
     ]
    }
   ],
   "source": [
    "history = atten_model.fit(train_ds, validation_data=valid_ds, batch_size=32, epochs=100, callbacks=[early_stopping, check_point], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x21df0766dc0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_model.load_weights('./weights/cross_atten/0302_223958/attention.t5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "scene_valid_ds = tf.data.experimental.load('./data/fullscene/valid_ds/')\n",
    "face_valid_ds  = tf.data.experimental.load('./data/faces/valid_ds/')\n",
    "audio_valid_ds = tf.data.experimental.load('./data/audio/valid_ds') \n",
    "text_valid_ds  = tf.data.experimental.load('./data/text/valid_ds/').batch(batch_size=32)\n",
    "\n",
    "scene_xvalid = scene_valid_ds.map(lambda x,y: x)\n",
    "face_xvalid  = face_valid_ds.map(lambda x,y: x)\n",
    "audio_xvalid = audio_valid_ds.map(lambda x,y: x)\n",
    "text_xvalid  = text_valid_ds.map(lambda x,y: x)\n",
    "y_valid      = scene_valid_ds.map(lambda x,y: y)\n",
    "\n",
    "valid_ds = tf.data.Dataset.zip(((scene_xvalid, face_xvalid, audio_xvalid, text_xvalid), y_valid)).prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([91.574974, 92.207054, 91.53847 , 91.64992 , 91.32228 ],\n",
       "       dtype=float32),\n",
       " 91.65853783488274)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "y_true = np.concatenate([y for x,y in valid_ds], axis=0)\n",
    "y_pred = atten_model.predict(valid_ds)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "(1-mae)*100, (1-np.mean(mae))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_test_ds = tf.data.experimental.load('../fullscene/test_ds/')\n",
    "face_test_ds  = tf.data.experimental.load('../faces/test_ds/')\n",
    "audio_test_ds = tf.data.experimental.load('../audio/test_ds') \n",
    "text_test_ds  = tf.data.experimental.load('../text/test_ds/').batch(batch_size=32)\n",
    "\n",
    "\n",
    "scene_xtest = scene_test_ds.map(lambda x,y: x)\n",
    "face_xtest  = face_test_ds.map(lambda x,y: x)\n",
    "audio_xtest = audio_test_ds.map(lambda x,y: x)\n",
    "text_xtest  = text_test_ds.map(lambda x,y: x)\n",
    "\n",
    "y_test      = scene_test_ds.map(lambda x,y: y)\n",
    "\n",
    "test_ds = tf.data.Dataset.zip(((scene_xtest, face_xtest, audio_xtest, text_xtest), y_test)).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([91.1931 , 92.16037, 91.49895, 91.25758, 91.33639], dtype=float32),\n",
       " 91.48927703499794)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.concatenate([y for x,y in test_ds], axis=0)\n",
    "y_pred = atten_model.predict(test_ds)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "(1-mae)*100, (1-np.mean(mae))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./histories/attention_cross.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfpy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
