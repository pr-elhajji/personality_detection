{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\21265\\anaconda3\\envs\\tfpy39\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "import tensorflow_addons as tfa \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from models import load_ef_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load EarlyFusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ef_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Scene_input (InputLayer)        [(None, 10, 224, 224 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Face_input (InputLayer)         [(None, 10, 224, 224 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Audio_input (InputLayer)        [(None, 15, 128)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Text_input (InputLayer)         [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "scene_model (Functional)        (None, 5)            102714949   Scene_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "face_model (Functional)         (None, 5)            102714949   Face_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "audio_model (Functional)        (None, 5)            2048613     Audio_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "text_model (Functional)         (None, 5)            1204033     Text_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_2 (Average)             (None, 5)            0           scene_model[0][0]                \n",
      "                                                                 face_model[0][0]                 \n",
      "                                                                 audio_model[0][0]                \n",
      "                                                                 text_model[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 193,967,856\n",
      "Trainable params: 6,550,656\n",
      "Non-trainable params: 187,417,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ef_model = load_ef_model()\n",
    "ef_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>,\n",
       " <PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Train\n",
    "scene_train_ds = tf.data.experimental.load('./data/fullscene/train_ds/')\n",
    "face_train_ds  = tf.data.experimental.load('./data/faces/train_ds/')\n",
    "audio_train_ds = tf.data.experimental.load('./data/audio/train_ds/')\n",
    "text_train_ds  = tf.data.experimental.load('./data/text/train_ds/').batch(batch_size=32)\n",
    "\n",
    "scene_xtrain = scene_train_ds.map(lambda x,y: x)\n",
    "face_xtrain  = face_train_ds.map(lambda x,y: x)\n",
    "audio_xtrain = audio_train_ds.map(lambda x,y: x)\n",
    "text__xtrain = text_train_ds.map(lambda x,y: x)\n",
    "y_train      = scene_train_ds.map(lambda x,y: y)\n",
    "\n",
    "train_ds = tf.data.Dataset.zip(((scene_xtrain, face_xtrain, audio_xtrain, text__xtrain), y_train)).shuffle(buffer_size=1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "# Valid\n",
    "scene_valid_ds = tf.data.experimental.load('./data/fullscene/valid_ds/')\n",
    "face_valid_ds  = tf.data.experimental.load('./data/faces/valid_ds/')\n",
    "audio_valid_ds = tf.data.experimental.load('./data/audio/valid_ds') \n",
    "text_valid_ds  = tf.data.experimental.load('./data/text/valid_ds/').batch(batch_size=32)\n",
    "\n",
    "scene_xvalid = scene_valid_ds.map(lambda x,y: x)\n",
    "face_xvalid  = face_valid_ds.map(lambda x,y: x)\n",
    "audio_xvalid = audio_valid_ds.map(lambda x,y: x)\n",
    "text_xvalid  = text_valid_ds.map(lambda x,y: x)\n",
    "y_valid      = scene_valid_ds.map(lambda x,y: y)\n",
    "\n",
    "valid_ds = tf.data.Dataset.zip(((scene_xvalid, face_xvalid, audio_xvalid, text_xvalid), y_valid)).shuffle(buffer_size=1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "188/188 [==============================] - 861s 4s/step - loss: 0.0044 - mae: 0.0522 - val_loss: 0.0109 - val_mae: 0.0826\n",
      "Epoch 2/100\n",
      "188/188 [==============================] - 842s 4s/step - loss: 0.0041 - mae: 0.0506 - val_loss: 0.0113 - val_mae: 0.0839\n",
      "Epoch 3/100\n",
      "188/188 [==============================] - 837s 4s/step - loss: 0.0040 - mae: 0.0502 - val_loss: 0.0114 - val_mae: 0.0845\n",
      "Epoch 4/100\n",
      "188/188 [==============================] - 830s 4s/step - loss: 0.0042 - mae: 0.0514 - val_loss: 0.0117 - val_mae: 0.0858\n",
      "Epoch 5/100\n",
      "188/188 [==============================] - 839s 4s/step - loss: 0.0042 - mae: 0.0510 - val_loss: 0.0120 - val_mae: 0.0872\n",
      "Epoch 6/100\n",
      "188/188 [==============================] - 835s 4s/step - loss: 0.0039 - mae: 0.0495 - val_loss: 0.0121 - val_mae: 0.0874\n",
      "Epoch 7/100\n",
      "188/188 [==============================] - 826s 4s/step - loss: 0.0037 - mae: 0.0478 - val_loss: 0.0122 - val_mae: 0.0880\n",
      "Epoch 8/100\n",
      "188/188 [==============================] - 596s 3s/step - loss: 0.0034 - mae: 0.0463 - val_loss: 0.0117 - val_mae: 0.0858\n",
      "Epoch 9/100\n",
      "188/188 [==============================] - 599s 3s/step - loss: 0.0032 - mae: 0.0447 - val_loss: 0.0125 - val_mae: 0.0887\n",
      "Epoch 10/100\n",
      "188/188 [==============================] - 833s 4s/step - loss: 0.0030 - mae: 0.0428 - val_loss: 0.0124 - val_mae: 0.0882\n",
      "Epoch 11/100\n",
      "188/188 [==============================] - 810s 4s/step - loss: 0.0028 - mae: 0.0415 - val_loss: 0.0123 - val_mae: 0.0881\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "t = datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=10, verbose=0)\n",
    "check_point    = keras.callbacks.ModelCheckpoint(filepath='./weights/ef/'+str(t)+'/ef.t5',\n",
    "                             monitor='val_mae',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             verbose=0)\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam()\n",
    "ef_model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "history = ef_model.fit(train_ds, validation_data=valid_ds, batch_size=32, epochs=100, callbacks=[early_stopping, check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "scene_valid_ds = tf.data.experimental.load('./data/fullscene/valid_ds/')\n",
    "face_valid_ds  = tf.data.experimental.load('./data/faces/valid_ds/')\n",
    "audio_valid_ds = tf.data.experimental.load('./data/audio/valid_ds') \n",
    "text_valid_ds  = tf.data.experimental.load('./data/text/valid_ds/').batch(batch_size=32)\n",
    "\n",
    "scene_xvalid = scene_valid_ds.map(lambda x,y: x)\n",
    "face_xvalid  = face_valid_ds.map(lambda x,y: x)\n",
    "audio_xvalid = audio_valid_ds.map(lambda x,y: x)\n",
    "text_xvalid  = text_valid_ds.map(lambda x,y: x)\n",
    "y_valid      = scene_valid_ds.map(lambda x,y: y)\n",
    "\n",
    "valid_ds = tf.data.Dataset.zip(((scene_xvalid, face_xvalid, audio_xvalid, text_xvalid), y_valid)).prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 122s 2s/step - loss: 0.0109 - mae: 0.0826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.74359366297722"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ef_model.load_weights('./weights/ef/0227_074102/ef.t5')\n",
    "loss, mae = ef_model.evaluate(valid_ds)\n",
    "(1-mae)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([91.630356, 92.24704 , 91.63761 , 91.83563 , 91.3673  ],\n",
       "       dtype=float32),\n",
       " 91.74358993768692)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.concatenate([y for x,y in valid_ds], axis=0)\n",
    "y_pred = ef_model.predict(valid_ds)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "(1-mae)*100, (1-np.mean(mae))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_test_ds = tf.data.experimental.load('./data/fullscene/test_ds/')\n",
    "face_test_ds  = tf.data.experimental.load('./data/faces/test_ds/')\n",
    "audio_test_ds = tf.data.experimental.load('./data/audio/test_ds') \n",
    "text_test_ds  = tf.data.experimental.load('./data/text/test_ds/').batch(batch_size=32)\n",
    "\n",
    "\n",
    "scene_xtest = scene_test_ds.map(lambda x,y: x)\n",
    "face_xtest  = face_test_ds.map(lambda x,y: x)\n",
    "audio_xtest = audio_test_ds.map(lambda x,y: x)\n",
    "text_xtest  = text_test_ds.map(lambda x,y: x)\n",
    "\n",
    "y_test      = scene_test_ds.map(lambda x,y: y)\n",
    "\n",
    "test_ds = tf.data.Dataset.zip(((scene_xtest, face_xtest, audio_xtest, text_xtest), y_test)).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 121s 2s/step - loss: 0.0109 - mae: 0.0829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.70725345611572"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    loss, mae = ef_model.evaluate(test_ds)\n",
    "(1-mae)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([91.49651, 92.27196, 91.80983, 91.43613, 91.52185], dtype=float32),\n",
       " 91.70725718140602)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.concatenate([y for x,y in test_ds], axis=0)\n",
    "y_pred = ef_model.predict(test_ds)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "(1-mae)*100, (1-np.mean(mae))*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfpy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
