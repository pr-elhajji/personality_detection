{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\21265\\anaconda3\\envs\\tfpy39\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "import tensorflow_addons as tfa \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from models import load_mf_model\n",
    "#import gensim.downloader as api\n",
    "#from utils import load_annotations, load_transcriptions, process_text, preprocess_text, loss_val_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>,\n",
       " <PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Train\n",
    "scene_train_ds = tf.data.experimental.load('./data/fullscene/train_ds/')\n",
    "face_train_ds  = tf.data.experimental.load('./data/faces/train_ds/')\n",
    "audio_train_ds = tf.data.experimental.load('./data/audio/train_ds/')\n",
    "text_train_ds  = tf.data.experimental.load('./data/text/train_ds/').batch(batch_size=32)\n",
    "\n",
    "scene_xtrain = scene_train_ds.map(lambda x,y: x)\n",
    "face_xtrain  = face_train_ds.map(lambda x,y: x)\n",
    "audio_xtrain = audio_train_ds.map(lambda x,y: x)\n",
    "text__xtrain = text_train_ds.map(lambda x,y: x)\n",
    "y_train      = scene_train_ds.map(lambda x,y: y)\n",
    "\n",
    "train_ds = tf.data.Dataset.zip(((scene_xtrain, face_xtrain, audio_xtrain, text__xtrain), y_train)).shuffle(buffer_size=1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "# Valid\n",
    "scene_valid_ds = tf.data.experimental.load('./data/fullscene/valid_ds/')\n",
    "face_valid_ds  = tf.data.experimental.load('./data/faces/valid_ds/')\n",
    "audio_valid_ds = tf.data.experimental.load('./data/audio/valid_ds') \n",
    "text_valid_ds  = tf.data.experimental.load('./data/text/valid_ds/').batch(batch_size=32)\n",
    "\n",
    "scene_xvalid = scene_valid_ds.map(lambda x,y: x)\n",
    "face_xvalid  = face_valid_ds.map(lambda x,y: x)\n",
    "audio_xvalid = audio_valid_ds.map(lambda x,y: x)\n",
    "text_xvalid  = text_valid_ds.map(lambda x,y: x)\n",
    "y_valid      = scene_valid_ds.map(lambda x,y: y)\n",
    "\n",
    "valid_ds = tf.data.Dataset.zip(((scene_xvalid, face_xvalid, audio_xvalid, text_xvalid), y_valid)).shuffle(buffer_size=1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_model = load_mf_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2a1caa41ee0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf_model.load_weights('../weights/mf_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 217s 2s/step - loss: 0.0120 - mae: 0.0871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.2941500544548"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, mae = mf_model.evaluate(valid_ds)\n",
    "(1-mae)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_test_ds = tf.data.experimental.load('./data/fullscene/test_ds/')\n",
    "face_test_ds  = tf.data.experimental.load('./data/faces/test_ds/')\n",
    "audio_test_ds = tf.data.experimental.load('./data/audio/test_ds') \n",
    "text_test_ds  = tf.data.experimental.load('./data/text/test_ds/').batch(batch_size=32)\n",
    "\n",
    "\n",
    "scene_xtest = scene_test_ds.map(lambda x,y: x)\n",
    "face_xtest  = face_test_ds.map(lambda x,y: x)\n",
    "audio_xtest = audio_test_ds.map(lambda x,y: x)\n",
    "text_xtest  = text_test_ds.map(lambda x,y: x)\n",
    "\n",
    "y_test      = scene_test_ds.map(lambda x,y: y)\n",
    "\n",
    "test_ds = tf.data.Dataset.zip(((scene_xtest, face_xtest, audio_xtest, text_xtest), y_test)).shuffle(buffer_size=1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 147s 2s/step - loss: 0.0116 - mae: 0.0858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.41548722982407"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, mae = mf_model.evaluate(test_ds)\n",
    "(1-mae)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>,\n",
       " <PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "t = datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=10, verbose=0)\n",
    "check_point    = keras.callbacks.ModelCheckpoint(filepath='./weights/mf/'+str(t)+'/mf.t5',\n",
    "                             monitor='val_mae',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             verbose=0)\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam()\n",
    "mf_model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "188/188 [==============================] - 650s 3s/step - loss: 0.0052 - mae: 0.0567 - val_loss: 0.0115 - val_mae: 0.0851\n",
      "Epoch 2/100\n",
      "188/188 [==============================] - 580s 3s/step - loss: 0.0046 - mae: 0.0536 - val_loss: 0.0117 - val_mae: 0.0862\n",
      "Epoch 3/100\n",
      "188/188 [==============================] - 581s 3s/step - loss: 0.0046 - mae: 0.0534 - val_loss: 0.0119 - val_mae: 0.0864\n",
      "Epoch 4/100\n",
      "188/188 [==============================] - 624s 3s/step - loss: 0.0046 - mae: 0.0539 - val_loss: 0.0117 - val_mae: 0.0863\n",
      "Epoch 5/100\n",
      "188/188 [==============================] - 642s 3s/step - loss: 0.0047 - mae: 0.0542 - val_loss: 0.0120 - val_mae: 0.0873\n",
      "Epoch 6/100\n",
      "188/188 [==============================] - 676s 3s/step - loss: 0.0048 - mae: 0.0548 - val_loss: 0.0122 - val_mae: 0.0882\n",
      "Epoch 7/100\n",
      "188/188 [==============================] - 572s 3s/step - loss: 0.0048 - mae: 0.0545 - val_loss: 0.0121 - val_mae: 0.0874\n",
      "Epoch 8/100\n",
      "188/188 [==============================] - 809s 4s/step - loss: 0.0048 - mae: 0.0548 - val_loss: 0.0121 - val_mae: 0.0873\n",
      "Epoch 9/100\n",
      "188/188 [==============================] - 811s 4s/step - loss: 0.0048 - mae: 0.0550 - val_loss: 0.0127 - val_mae: 0.0895\n",
      "Epoch 10/100\n",
      "188/188 [==============================] - 823s 4s/step - loss: 0.0049 - mae: 0.0554 - val_loss: 0.0119 - val_mae: 0.0864\n",
      "Epoch 11/100\n",
      "188/188 [==============================] - 818s 4s/step - loss: 0.0047 - mae: 0.0544 - val_loss: 0.0120 - val_mae: 0.0872\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = mf_model.fit(train_ds, validation_data=valid_ds, batch_size=32, epochs=100, callbacks=[early_stopping, check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x226d651ad90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf_model.load_weights('./weights/mf/0226_221130/mf.t5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "scene_valid_ds = tf.data.experimental.load('./data/fullscene/valid_ds/')\n",
    "face_valid_ds  = tf.data.experimental.load('./data/faces/valid_ds/')\n",
    "audio_valid_ds = tf.data.experimental.load('./data/audio/valid_ds') \n",
    "text_valid_ds  = tf.data.experimental.load('./data/text/valid_ds/').batch(batch_size=32)\n",
    "\n",
    "scene_xvalid = scene_valid_ds.map(lambda x,y: x)\n",
    "face_xvalid  = face_valid_ds.map(lambda x,y: x)\n",
    "audio_xvalid = audio_valid_ds.map(lambda x,y: x)\n",
    "text_xvalid  = text_valid_ds.map(lambda x,y: x)\n",
    "y_valid      = scene_valid_ds.map(lambda x,y: y)\n",
    "\n",
    "valid_ds = tf.data.Dataset.zip(((scene_xvalid, face_xvalid, audio_xvalid, text_xvalid), y_valid)).prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 221s 2s/step - loss: 0.0120 - mae: 0.0867\n"
     ]
    }
   ],
   "source": [
    "loss, mae = mf_model.evaluate(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([91.35425, 92.05732, 91.32551, 91.67088, 91.0415 ], dtype=float32),\n",
       " 91.48988947272301)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "y_true = np.concatenate([y for x,y in valid_ds], axis=0)\n",
    "y_pred = mf_model.predict(valid_ds)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "(1-mae)*100, (1-np.mean(mae))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_test_ds = tf.data.experimental.load('./data/fullscene/test_ds/')\n",
    "face_test_ds  = tf.data.experimental.load('./data/faces/test_ds/')\n",
    "audio_test_ds = tf.data.experimental.load('./data/audio/test_ds') \n",
    "text_test_ds  = tf.data.experimental.load('./data/text/test_ds/').batch(batch_size=32)\n",
    "\n",
    "\n",
    "scene_xtest = scene_test_ds.map(lambda x,y: x)\n",
    "face_xtest  = face_test_ds.map(lambda x,y: x)\n",
    "audio_xtest = audio_test_ds.map(lambda x,y: x)\n",
    "text_xtest  = text_test_ds.map(lambda x,y: x)\n",
    "\n",
    "y_test      = scene_test_ds.map(lambda x,y: y)\n",
    "\n",
    "test_ds = tf.data.Dataset.zip(((scene_xtest, face_xtest, audio_xtest, text_xtest), y_test)).prefetch(buffer_size=AUTOTUNE) #.shuffle(buffer_size=1000)\n",
    "\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([91.42722, 92.14998, 91.67239, 91.44349, 91.34789], dtype=float32),\n",
       " 91.6081964969635)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.concatenate([y for x,y in test_ds], axis=0)\n",
    "y_pred = mf_model.predict(test_ds)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "(1-mae)*100, (1-np.mean(mae))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.60818979144096"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-mae)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./histories/mf.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfpy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
