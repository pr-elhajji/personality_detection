# A Deep Multimodal Fusion Method for Personality Traits prediction 
we propose a novel deep multimodal fusion for predicting personality traits from diverse data modalities, including text, audio, and visual inputs. Our proposed method extracts complex patterns and features from these multimodal data sources using advanced deep learning techniques including Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, and Vision Transformer (ViT). Specifically, we use pre-trained models ViT-B16 and VGG16 for visual feature extraction, VGGish for audio feature extraction, and GloVe for text analysis. Additionally, we investigate the potential benefits of using self-attention and cross-attention mechanisms to provide accurate predictions regarding an individualâ€™s personality traits. The method we propose combines information from several modalities using various fusion techniques, improving the predictive capability of the model. 

# Contributeur:

Ayoub Ouarka (ORCID)
Polydisciplinary faculty of Taroudant,  Ibnou Zohr University, Morocco.
Tarek AIT BAHA (ORCiD)
IRF-SIC Laboratory, Ibnou Zohr University, Agadir, Morocco.
Youssef ES-SAADY (ORCiD)
Polydisciplinary faculty of Taroudant,  Ibnou Zohr University, Morocco.
IRF-SIC Laboratory, Ibnou Zohr University, Agadir, Morocco
Mohamed EL HAJJI (ORCiD)
Regional Center for Education and Training Professions - Souss Massa, Morocco 
IRF-SIC Laboratory, Ibnou Zohr University, Agadir, Morocco,

