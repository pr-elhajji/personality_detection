{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\21265\\anaconda3\\envs\\tfpy39\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "import tensorflow_addons as tfa \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from models import load_scene_model, load_face_model, load_audio_model, load_text_glove_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_model = load_scene_model()\n",
    "face_model  = load_face_model()\n",
    "audio_model = load_audio_model()\n",
    "text_model  = load_text_glove_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"scene_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Scene_Input (InputLayer)        [(None, 10, 224, 224 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Scene_Rescaling (TimeDistribute (None, 10, 224, 224, 0           Scene_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Scene_time_distributed (TimeDis (None, 10, 224, 224, 0           Scene_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Scene_vgg16 (TimeDistributed)   (None, 10, 512)      14714688    Scene_Rescaling[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Scene_time_distributed_1 (TimeD (None, 10, 768)      85798656    Scene_time_distributed[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Scene_lstm (LSTM)               (None, 10, 128)      328192      Scene_vgg16[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Scene_lstm_2 (LSTM)             (None, 10, 128)      459264      Scene_time_distributed_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Scene_lstm_1 (LSTM)             (None, 64)           49408       Scene_lstm[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Scene_lstm_3 (LSTM)             (None, 64)           49408       Scene_lstm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dropout (Dropout)         (None, 64)           0           Scene_lstm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dropout_1 (Dropout)       (None, 64)           0           Scene_lstm_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dense (Dense)             (None, 1024)         66560       Scene_dropout[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dense_2 (Dense)           (None, 1024)         66560       Scene_dropout_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dense_1 (Dense)           (None, 512)          524800      Scene_dense[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dense_3 (Dense)           (None, 512)          524800      Scene_dense_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Scene_average (Average)         (None, 512)          0           Scene_dense_1[0][0]              \n",
      "                                                                 Scene_dense_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dense_4 (Dense)           (None, 256)          131328      Scene_average[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dropout_2 (Dropout)       (None, 256)          0           Scene_dense_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Scene_dense_5 (Dense)           (None, 5)            1285        Scene_dropout_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 102,714,949\n",
      "Trainable params: 0\n",
      "Non-trainable params: 102,714,949\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in scene_model.layers:\n",
    "    layer.trainable = False \n",
    "    layer._name = 'Scene_' + layer._name\n",
    "scene_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"face_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Face_Input (InputLayer)         [(None, 10, 224, 224 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Face_Rescaling (TimeDistributed (None, 10, 224, 224, 0           Face_Input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Face_time_distributed_2 (TimeDi (None, 10, 224, 224, 0           Face_Input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Face_vgg16 (TimeDistributed)    (None, 10, 512)      14714688    Face_Rescaling[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Face_time_distributed_3 (TimeDi (None, 10, 768)      85798656    Face_time_distributed_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Face_lstm_4 (LSTM)              (None, 10, 128)      328192      Face_vgg16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Face_lstm_6 (LSTM)              (None, 10, 128)      459264      Face_time_distributed_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Face_lstm_5 (LSTM)              (None, 64)           49408       Face_lstm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Face_lstm_7 (LSTM)              (None, 64)           49408       Face_lstm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Face_dropout_3 (Dropout)        (None, 64)           0           Face_lstm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Face_dropout_4 (Dropout)        (None, 64)           0           Face_lstm_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Face_dense_6 (Dense)            (None, 1024)         66560       Face_dropout_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Face_dense_8 (Dense)            (None, 1024)         66560       Face_dropout_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Face_dense_7 (Dense)            (None, 512)          524800      Face_dense_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Face_dense_9 (Dense)            (None, 512)          524800      Face_dense_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Face_average_1 (Average)        (None, 512)          0           Face_dense_7[0][0]               \n",
      "                                                                 Face_dense_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Face_dense_10 (Dense)           (None, 256)          131328      Face_average_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Face_dropout_5 (Dropout)        (None, 256)          0           Face_dense_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Face_dense_11 (Dense)           (None, 5)            1285        Face_dropout_5[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 102,714,949\n",
      "Trainable params: 0\n",
      "Non-trainable params: 102,714,949\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in face_model.layers:\n",
    "    layer.trainable = False \n",
    "    layer._name = 'Face_' + layer._name\n",
    "face_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"audio_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Audio_input_2 (InputLayer)   [(None, 15, 128)]         0         \n",
      "_________________________________________________________________\n",
      "Audio_conv1d (Conv1D)        (None, 14, 32)            8224      \n",
      "_________________________________________________________________\n",
      "Audio_dropout_6 (Dropout)    (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "Audio_conv1d_1 (Conv1D)      (None, 13, 64)            4160      \n",
      "_________________________________________________________________\n",
      "Audio_dropout_7 (Dropout)    (None, 13, 64)            0         \n",
      "_________________________________________________________________\n",
      "Audio_lstm_8 (LSTM)          (None, 13, 512)           1181696   \n",
      "_________________________________________________________________\n",
      "Audio_lstm_9 (LSTM)          (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "Audio_dense_12 (Dense)       (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "Audio_dropout_8 (Dropout)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Audio_dense_13 (Dense)       (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 2,048,613\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,048,613\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in audio_model.layers:\n",
    "    layer.trainable = False\n",
    "    layer._name = 'Audio_' + layer._name\n",
    "audio_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Text_input_3 (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Text_embedding (Embedding)      (None, 50, 100)      1105200     Text_input_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Text_conv1d_2 (Conv1D)          (None, 48, 16)       4816        Text_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Text_conv1d_4 (Conv1D)          (None, 48, 32)       9632        Text_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Text_conv1d_3 (Conv1D)          (None, 46, 8)        392         Text_conv1d_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Text_conv1d_5 (Conv1D)          (None, 46, 16)       1552        Text_conv1d_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Text_flatten (Flatten)          (None, 368)          0           Text_conv1d_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Text_flatten_1 (Flatten)        (None, 736)          0           Text_conv1d_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Text_dense_14 (Dense)           (None, 50)           18450       Text_flatten[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Text_dense_15 (Dense)           (None, 50)           36850       Text_flatten_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Text_concatenate (Concatenate)  (None, 100)          0           Text_dense_14[0][0]              \n",
      "                                                                 Text_dense_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Text_dense_16 (Dense)           (None, 256)          25856       Text_concatenate[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Text_dense_17 (Dense)           (None, 5)            1285        Text_dense_16[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,204,033\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,204,033\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in text_model.layers:\n",
    "    layer.trainable = False \n",
    "    layer._name = 'Text_' + layer._name\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene shape: (None, 10, 64)\n",
      "Face shape: (None, 10, 64)\n",
      "Audio shape: (None, 13, 64)\n",
      "Text shape: (None, 46, 64)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Scene_Input (InputLayer)        [(None, 10, 224, 224 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Face_Input (InputLayer)         [(None, 10, 224, 224 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Audio_input_2 (InputLayer)      [(None, 15, 128)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Text_input_3 (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Scene_time_distributed (TimeDis (None, 10, 224, 224, 0           Scene_Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Face_time_distributed_2 (TimeDi (None, 10, 224, 224, 0           Face_Input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Audio_conv1d (Conv1D)           (None, 14, 32)       8224        Audio_input_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Text_embedding (Embedding)      (None, 50, 100)      1105200     Text_input_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Scene_time_distributed_1 (TimeD (None, 10, 768)      85798656    Scene_time_distributed[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Face_time_distributed_3 (TimeDi (None, 10, 768)      85798656    Face_time_distributed_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Audio_dropout_6 (Dropout)       (None, 14, 32)       0           Audio_conv1d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Text_conv1d_4 (Conv1D)          (None, 48, 32)       9632        Text_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Scene_lstm_2 (LSTM)             (None, 10, 128)      459264      Scene_time_distributed_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Face_lstm_6 (LSTM)              (None, 10, 128)      459264      Face_time_distributed_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Audio_conv1d_1 (Conv1D)         (None, 13, 64)       4160        Audio_dropout_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Text_conv1d_5 (Conv1D)          (None, 46, 16)       1552        Text_conv1d_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 10, 64)       8256        Scene_lstm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 10, 64)       8256        Face_lstm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Audio_dropout_7 (Dropout)       (None, 13, 64)       0           Audio_conv1d_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 46, 64)       1088        Text_conv1d_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 10, 64)       33216       dense_18[0][0]                   \n",
      "                                                                 dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (None, 10, 64)       33216       dense_19[0][0]                   \n",
      "                                                                 dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_2 (MultiHe (None, 13, 64)       33216       Audio_dropout_7[0][0]            \n",
      "                                                                 Audio_dropout_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_3 (MultiHe (None, 46, 64)       33216       dense_20[0][0]                   \n",
      "                                                                 dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 79, 64)       0           multi_head_attention[0][0]       \n",
      "                                                                 multi_head_attention_1[0][0]     \n",
      "                                                                 multi_head_attention_2[0][0]     \n",
      "                                                                 multi_head_attention_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 64)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 128)          8320        global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128)          0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 5)            645         dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 173,804,037\n",
      "Trainable params: 159,429\n",
      "Non-trainable params: 173,644,608\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "scene_inputs = keras.layers.Input(shape=(10,224,224,3), name='Scene_input')\n",
    "face_inputs  = keras.layers.Input(shape=(10,224,224,3), name='Face_input')\n",
    "audio_inputs = keras.layers.Input(shape=(15,128), name='Audio_input')\n",
    "text_inputs  = keras.layers.Input(shape=(50), name='Text_input')\n",
    "\n",
    "x = scene_model.layers[-13].output\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "x = keras.layers.MultiHeadAttention(num_heads=2, key_dim=64)(x,x)\n",
    "print('Scene shape:', x.shape)\n",
    "\n",
    "y = face_model.layers[-13].output\n",
    "y = keras.layers.Dense(64, activation='relu')(y)\n",
    "y = keras.layers.MultiHeadAttention(num_heads=2,key_dim=64)(y,y)\n",
    "print('Face shape:', y.shape)\n",
    "\n",
    "z = audio_model.layers[-6].output\n",
    "z = keras.layers.MultiHeadAttention(num_heads=2,key_dim=64)(z,z)\n",
    "print('Audio shape:', z.shape)\n",
    "\n",
    "w = text_model.layers[-8].output\n",
    "w = keras.layers.Dense(64, activation='relu')(w)\n",
    "w = keras.layers.MultiHeadAttention(num_heads=2,key_dim=64)(w,w)\n",
    "print('Text shape:', w.shape)\n",
    "\n",
    "o = keras.layers.Concatenate(axis=1)([x,y,z,w])\n",
    "\n",
    "o = keras.layers.GlobalAveragePooling1D()(o)\n",
    "\n",
    "\n",
    "o = keras.layers.Dense(128, activation='relu')(o)\n",
    "o = keras.layers.Dropout(0.2)(o)\n",
    "o = keras.layers.Dense(5, activation='sigmoid')(o)\n",
    "# x.shape, y.shape, z.shape, w.shape, o.shape\n",
    "\n",
    "\n",
    "\n",
    "atten_model = keras.models.Model(inputs=[scene_model.input, face_model.input, audio_model.input, text_model.input], outputs=o)\n",
    "atten_model.compile(loss='mse', optimizer=tfa.optimizers.RectifiedAdam(), metrics=['mae'])\n",
    "\n",
    "\n",
    "# keras.utils.plot_model(atten_model, show_shapes=True)\n",
    "atten_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "t = datetime.datetime.now().strftime(\"%m%d_%H%M%S\")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=10, verbose=0)\n",
    "check_point    = keras.callbacks.ModelCheckpoint(filepath='./weights/self_attention/'+str(t)+'/attention.t5',\n",
    "                             monitor='val_mae',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>,\n",
       " <PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Train\n",
    "scene_train_ds = tf.data.experimental.load('./data/fullscene/train_ds/')\n",
    "face_train_ds  = tf.data.experimental.load('./data/faces/train_ds/')\n",
    "audio_train_ds = tf.data.experimental.load('./data/audio/train_ds/')\n",
    "text_train_ds  = tf.data.experimental.load('./data/text/train_ds/').batch(batch_size=32)\n",
    "\n",
    "scene_xtrain = scene_train_ds.map(lambda x,y: x)\n",
    "face_xtrain  = face_train_ds.map(lambda x,y: x)\n",
    "audio_xtrain = audio_train_ds.map(lambda x,y: x)\n",
    "text__xtrain = text_train_ds.map(lambda x,y: x)\n",
    "y_train      = scene_train_ds.map(lambda x,y: y)\n",
    "\n",
    "train_ds = tf.data.Dataset.zip(((scene_xtrain, face_xtrain, audio_xtrain, text__xtrain), y_train)).shuffle(buffer_size=1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "# Valid\n",
    "scene_valid_ds = tf.data.experimental.load('./data/fullscene/valid_ds/')\n",
    "face_valid_ds  = tf.data.experimental.load('./data/faces/valid_ds/')\n",
    "audio_valid_ds = tf.data.experimental.load('./data/audio/valid_ds') \n",
    "text_valid_ds  = tf.data.experimental.load('./data/text/valid_ds/').batch(batch_size=32)\n",
    "\n",
    "scene_xvalid = scene_valid_ds.map(lambda x,y: x)\n",
    "face_xvalid  = face_valid_ds.map(lambda x,y: x)\n",
    "audio_xvalid = audio_valid_ds.map(lambda x,y: x)\n",
    "text_xvalid  = text_valid_ds.map(lambda x,y: x)\n",
    "y_valid      = scene_valid_ds.map(lambda x,y: y)\n",
    "\n",
    "valid_ds = tf.data.Dataset.zip(((scene_xvalid, face_xvalid, audio_xvalid, text_xvalid), y_valid)).shuffle(buffer_size=1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "188/188 [==============================] - 523s 2s/step - loss: 0.0160 - mae: 0.1004 - val_loss: 0.0118 - val_mae: 0.0862\n",
      "Epoch 2/100\n",
      "188/188 [==============================] - 471s 2s/step - loss: 0.0076 - mae: 0.0694 - val_loss: 0.0115 - val_mae: 0.0848\n",
      "Epoch 3/100\n",
      "188/188 [==============================] - 682s 3s/step - loss: 0.0065 - mae: 0.0641 - val_loss: 0.0114 - val_mae: 0.0845\n",
      "Epoch 4/100\n",
      "188/188 [==============================] - 619s 3s/step - loss: 0.0060 - mae: 0.0616 - val_loss: 0.0112 - val_mae: 0.0841\n",
      "Epoch 5/100\n",
      "188/188 [==============================] - 629s 3s/step - loss: 0.0058 - mae: 0.0603 - val_loss: 0.0114 - val_mae: 0.0847\n",
      "Epoch 6/100\n",
      "188/188 [==============================] - 620s 3s/step - loss: 0.0055 - mae: 0.0589 - val_loss: 0.0115 - val_mae: 0.0850\n",
      "Epoch 7/100\n",
      "188/188 [==============================] - 632s 3s/step - loss: 0.0054 - mae: 0.0580 - val_loss: 0.0112 - val_mae: 0.0839\n",
      "Epoch 8/100\n",
      "188/188 [==============================] - 620s 3s/step - loss: 0.0052 - mae: 0.0572 - val_loss: 0.0110 - val_mae: 0.0832\n",
      "Epoch 9/100\n",
      "188/188 [==============================] - 626s 3s/step - loss: 0.0050 - mae: 0.0563 - val_loss: 0.0119 - val_mae: 0.0866\n",
      "Epoch 10/100\n",
      "188/188 [==============================] - 637s 3s/step - loss: 0.0050 - mae: 0.0558 - val_loss: 0.0115 - val_mae: 0.0853\n",
      "Epoch 11/100\n",
      "188/188 [==============================] - 622s 3s/step - loss: 0.0048 - mae: 0.0552 - val_loss: 0.0116 - val_mae: 0.0857\n",
      "Epoch 12/100\n",
      "188/188 [==============================] - 642s 3s/step - loss: 0.0048 - mae: 0.0548 - val_loss: 0.0114 - val_mae: 0.0847\n",
      "Epoch 13/100\n",
      "188/188 [==============================] - 635s 3s/step - loss: 0.0046 - mae: 0.0540 - val_loss: 0.0120 - val_mae: 0.0870\n",
      "Epoch 14/100\n",
      "188/188 [==============================] - 636s 3s/step - loss: 0.0045 - mae: 0.0533 - val_loss: 0.0115 - val_mae: 0.0852\n",
      "Epoch 15/100\n",
      "188/188 [==============================] - 625s 3s/step - loss: 0.0045 - mae: 0.0530 - val_loss: 0.0119 - val_mae: 0.0867\n",
      "Epoch 16/100\n",
      "188/188 [==============================] - 616s 3s/step - loss: 0.0044 - mae: 0.0527 - val_loss: 0.0120 - val_mae: 0.0873\n",
      "Epoch 17/100\n",
      "188/188 [==============================] - 620s 3s/step - loss: 0.0043 - mae: 0.0519 - val_loss: 0.0120 - val_mae: 0.0870\n",
      "Epoch 18/100\n",
      "188/188 [==============================] - 634s 3s/step - loss: 0.0042 - mae: 0.0515 - val_loss: 0.0118 - val_mae: 0.0863\n"
     ]
    }
   ],
   "source": [
    "history = atten_model.fit(train_ds, validation_data=valid_ds, batch_size=4, epochs=100, callbacks=[early_stopping, check_point], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x21df0766dc0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_model.load_weights('./weights/self_attention/0301_223332/attention.t5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "scene_valid_ds = tf.data.experimental.load('./data/fullscene/valid_ds/')\n",
    "face_valid_ds  = tf.data.experimental.load('./data/faces/valid_ds/')\n",
    "audio_valid_ds = tf.data.experimental.load('./data/audio/valid_ds') \n",
    "text_valid_ds  = tf.data.experimental.load('./data/text/valid_ds/').batch(batch_size=32)\n",
    "\n",
    "scene_xvalid = scene_valid_ds.map(lambda x,y: x)\n",
    "face_xvalid  = face_valid_ds.map(lambda x,y: x)\n",
    "audio_xvalid = audio_valid_ds.map(lambda x,y: x)\n",
    "text_xvalid  = text_valid_ds.map(lambda x,y: x)\n",
    "y_valid      = scene_valid_ds.map(lambda x,y: y)\n",
    "\n",
    "valid_ds = tf.data.Dataset.zip(((scene_xvalid, face_xvalid, audio_xvalid, text_xvalid), y_valid)).prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([91.53201, 92.27915, 91.48858, 91.7785 , 91.33944], dtype=float32),\n",
       " 91.68353825807571)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "y_true = np.concatenate([y for x,y in valid_ds], axis=0)\n",
    "y_pred = atten_model.predict(valid_ds)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "(1-mae)*100, (1-np.mean(mae))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, 10, 224, 224, 3), (None, 10, 224, 224, 3), (None, 15, 128), (None, 50)), (None, 5)), types: ((tf.float32, tf.float32, tf.float32, tf.int32), tf.float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_test_ds = tf.data.experimental.load('./data/fullscene/test_ds/')\n",
    "face_test_ds  = tf.data.experimental.load('./data/faces/test_ds/')\n",
    "audio_test_ds = tf.data.experimental.load('./data/audio/test_ds') \n",
    "text_test_ds  = tf.data.experimental.load('./data/text/test_ds/').batch(batch_size=32)\n",
    "\n",
    "\n",
    "scene_xtest = scene_test_ds.map(lambda x,y: x)\n",
    "face_xtest  = face_test_ds.map(lambda x,y: x)\n",
    "audio_xtest = audio_test_ds.map(lambda x,y: x)\n",
    "text_xtest  = text_test_ds.map(lambda x,y: x)\n",
    "\n",
    "y_test      = scene_test_ds.map(lambda x,y: y)\n",
    "\n",
    "test_ds = tf.data.Dataset.zip(((scene_xtest, face_xtest, audio_xtest, text_xtest), y_test)).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([91.289116, 92.245865, 91.66372 , 91.486595, 91.36632 ],\n",
       "       dtype=float32),\n",
       " 91.61032140254974)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.concatenate([y for x,y in test_ds], axis=0)\n",
    "y_pred = atten_model.predict(test_ds)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "(1-mae)*100, (1-np.mean(mae))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./histories/attention_conc.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfpy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
